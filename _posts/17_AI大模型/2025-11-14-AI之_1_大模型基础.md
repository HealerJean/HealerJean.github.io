---
title: AI之_1_大模型基础
date: 2025-11-14 00:00:00
tags: 
- AI
category: 
- AI
description: AI之_1_大模型基础
---

**前言**     

 Github：[https://github.com/HealerJean](https://github.com/HealerJean)         

 博客：[http://blog.healerjean.com](http://HealerJean.github.io)          



# 一、名词解释

## 1、`LLM` 大语言模型

**解释：**大语言模型（`Large` `Language` `Model`，简称 `LLM`）是一种基于深度学习的人工智能系统，专门用于理解和生成人类语言。它通过在海量文本数据上进行训练，学习语言的统计规律和语义结构，从而能够回答问题、创作文本、翻译语言，甚至进行逻辑推理。    

`LLM` 的“大”主要体现在参数量巨大——从数十亿到数千亿不等。这种规模带来了所谓的“涌现能力”：当模型足够大时，它会突然展现出小模型不具备的理解力、泛化能力和上下文推理能力。就像你描述的那样，给它一个输入（“刺激”），它会根据内部学到的知识和概率分布，生成一个看似合理、有时甚至富有创意的回应。而且，即使是相同的输入，由于采样策略或随机性，输出也可能略有不同。



**关键特点**：

- 理解和生成自然语言
- 回答基于概率和训练数据
- 不具备记忆、目标或行动能力（除非被设计成有）
  - **举例说明**：
    - **你问 `LLM`**：“北京明天天气怎么样？”
    - **`LLM` 回答**：“抱歉，我无法获取实时天气信息。”



## 2、`Agent`：智能体

解释：`Agent`（智能体），它是构建在 `LLM` 之上的更高级系统。如果说 `LLM` 是“大脑”，能理解与表达，那么 `Agent` 就是“有行动能力的个体”。它不仅能理解用户意图，还能结合历史对话、用户身份、当前环境等信息，自主规划任务、调用工具（比如查天气、订机票）、做出决策并执行操作，实现端到端的智能服务。可以说，`Agent` 让 `LLM` 从“会说话”进化到“会做事”。

`Agent` 是在 LLM 基础上构建的“智能系统”。它不仅理解语言，还能**自主规划、调用工具、执行任务**，就像一个有执行力的助理。

**关键特点**：

- 能感知上下文（你是谁、之前聊过什么）
- 能设定目标（比如“帮我订一张机票”）
- 能调用外部工具（查天气、发邮件、操作数据库等）
- 能分步骤完成复杂任务

**举例说明**：：**你对 `Agent` 说**：“我想知道北京明天天气，如果下雨就提醒我带伞。”，**Agent 会这样做**：

- **理解意图**：用户想知道天气，并有条件判断；

- **调用工具**：自动连接天气 `API`，获取北京明天的预报；

- **判断结果**：发现预报是“中雨”；

- **执行动作**：回复你：“北京明天有雨，记得带伞！”



## 3、`MCP`：模型上下文协议

> **`MCP` 是 Agent 与外部世界沟通的“语言规范”**。  
>
> > **`Agent` 是“谁在做事”，`MCP` 是“怎么规范地做事”。`MCP` 为 `Agent` 提供了与外部世界安全、高效、标准化交互的桥梁。**

| 角色        | 职责                                                         | 依赖关系                                           |
| ----------- | ------------------------------------------------------------ | -------------------------------------------------- |
| **`Agent`** | 决策者 + 执行者：理解用户意图 → 规划步骤 → 调用工具 → 整合结果 | **需要 MCP（或类似协议）来规范它如何调用外部工具** |
| **`MCP`**   | 通信规则制定者：定义“怎么调用工具”“怎么传参数”“怎么返回结果” | **为 Agent 提供标准化、安全的工具调用接口**        |

举例说明：假设你有一个 **旅行规划 `Agent`**，任务是：“帮我订一张下周从北京到上海的 `cheapest` 机票。”

没有 `MCP`（传统方式）：

- 开发者要为这个 `Agent` **硬编码**如何调用某个特定机票 `API`；
- 如果换一个航班平台，就得重写代码；
- 安全性和错误处理也各不相同。



## 4、`RAG`：检索增强生成

> `RAG`（`Retrieval-Augmented Generation`，检索增强生成）是一种结合了**信息检索**（Information Retrieval）与**生成式语言模型**（`Generative Language Model`）的技术框架，旨在提升大语言模型在回答问题时的**准确性、时效性和可解释性**。

### 1）`RAG` 的基本原理

> 在生成答案之前，先从外部知识源（如文档库、数据库、网页等）中检索出与用户问题相关的信息，然后将这些信息作为上下文输入给生成模型，从而生成更准确、可靠的回答。

- **检索阶段**
  - 用户输入一个问题（query）。
  - 使用检索模型（如 BM25、DPR、ColBERT 或向量数据库中的嵌入检索）从大规模文档集合中找出最相关的若干文档片段
- **生成阶段**
  - 将原始问题 + 检索到的相关文档作为提示（`prompt`）输入给大语言模型（如 `LLaMA`、`GPT`、`Qwen` 等）。
  - 模型基于这些上下文生成最终答案。



### 2）`RAG` 的优势

- **减少幻觉**（`Hallucination`）：模型依赖真实检索到的证据，而非仅靠内部参数记忆。
- **知识可更新**：只需更新外部知识库，无需重新训练模型。
- **可解释性强**：可以展示引用来源（“根据以下文档……”）。
- **适用于专业领域**：如医疗、法律、金融等需要高准确性的场景。



### 3）典型应用场景

- 企业知识库问答系统（如客服、内部文档查询）
- 学术研究助手（基于论文库回答问题）
- 实时新闻或政策问答（结合最新数据）
- 智能客服/虚拟助手（结合产品手册）





# 二、`Ollama`

> `Ollama` 是一个开源项目，旨在简化在本地设备（如笔记本电脑或个人服务器）上运行大型语言模型（LLMs）的过程。它最初由 Jeffrey Huang 和其他贡献者开发，目标是让开发者、研究人员和爱好者能够轻松地下载、运行和与各种开源大模型进行交互，而无需依赖云端服务。

## 1、主要特点：

- **本地运行**： `Ollama` 允许用户在自己的机器上直接运行 LLM，无需联网，保障数据隐私和安全性。
- **支持多种模型**： 支持包括 `Llama`、`Llama2`、`Llama3`、`Mistral`、`Gemma`、`Phi`、`Qwen` 等主流开源模型
- **一键部署**： 通过简单的命令（如 `ollama run llama3`），即可自动下载并运行指定模型，极大降低使用门槛。
- **跨平台支持**： 提供 `macOS`、Linux 和 Windows（通过 WSL 或原生支持）版本。
- **API 接口**： `Ollama` 提供 RESTful API，方便集成到其他应用中，例如 Web 应用、桌面工具或自动化脚本。
- **模型管理**： 用户可以轻松列出、拉取、删除和切换模型，例如：



## 2、模型管理

http://localhost:11434

```sh
ollama list

ollama pull qwen:7b

ollama run llama3

ollama rm llama2
```



## 3、`FAQ`

### 1）`Ollama` 的“强大管理能力”从何而来？

> `Ollama` 的“强大”主要体现在 **易用性、抽象封装和本地运行优化**，而不是依赖云端。具体来说：

- **模型已经“训练好了”**

  - 像 `qwen:7b` 这类模型，是在阿里云的超级计算机上用海量数据**提前训练好**的。

  - `Ollama` 只是把训练好的“成品大脑”（权重文件）下载到你电脑上。

  - 所以推理（即回答问题）时，**不需要再联网学习或查询**，直接用已有知识生成答案。

- **本地推理引擎高度优化**

  - `Ollama` 内部集成了高效的推理后端（如 llama.cpp），专门针对 CPU/GPU 做了优化。

  - 支持 Metal（`Mac`）、CUDA（NVIDIA）、AVX 等硬件加速，让大模型在笔记本上也能跑得动。

  - 所有计算都在你设备上完成 → **无需网络，保护隐私，响应快**。

- **“管理功能”是本地程序逻辑，不依赖云端**

  - `ollama list`、`ollama run`、`ollama rm` 这些命令只是在操作你本地的文件和进程。

  - 模型元数据、配置、缓存都存在你电脑上（如 `~/.ollama/` 目录）。

  - 就像 Docker 管理镜像一样：拉取一次，之后全本地操作。

- **API 也是本地服务**

  - 当你运行 `ollama serve`（默认自动启动），它会在 `http://localhost:11434` 启动一个本地 Web 服务。

  - 其他程序（如 Python 脚本、VS Code 插件）通过这个本地 API 与模型通信，**全程不经过互联网**。



### 2）为什么有些 `AI` 必须联网

> Ollama 的“强大”不是靠实时联网获取能力，而是：**把强大的 AI 模型“装进你的电脑”，让你拥有一个私有的、随时可用的 AI 助手。**

- **联网 `AI`**：像打电话问专家——每次都要拨号（联网），等对方回答。
- **`Ollama` 本地模型**：像请了一位专家住你家里——不用打电话，随时面对面交流。

| 类型                               | 是否需要联网 | 原因                                   |
| ---------------------------------- | ------------ | -------------------------------------- |
| **`Ollama`（本地 `LLM`）**         | 仅下载时需要 | 模型完整部署在本地                     |
| **`ChatGPT` / 文心一言 / 通义app** | 必须联网     | 模型在服务器上，你的输入要传到云端处理 |



### 3）**只有几个 GB 的文件，却能“记住”海量知识、回答各种问题**

> **几个 `GB` 不是“知识的总量”，而是“生成知识的能力”的压缩包**

| 原因             | 说明                                             |
| ---------------- | ------------------------------------------------ |
| **参数编码知识** | 知识以权重形式分布式存储在 70 亿参数中           |
| **高度压缩**     | **压缩冗余、提取共性**，用少量参数捕捉大量语义。 |
| **泛化能力**     | 学的是“规律”而非“原文”，能举一反三               |
| **语言冗余低**   | 模型只需捕捉核心语义，无需存储全部文本           |



# 二、`Spint AI` + `Ollama`















![ContactAuthor](https://raw.githubusercontent.com/HealerJean/HealerJean.github.io/master/assets/img/artical_bottom.jpg)



<!-- Gitalk 评论 start  -->

<link rel="stylesheet" href="https://unpkg.com/gitalk/dist/gitalk.css">

<script src="https://unpkg.com/gitalk@latest/dist/gitalk.min.js"></script> 
<div id="gitalk-container"></div>    
 <script type="text/javascript">
    var gitalk = new Gitalk({
		clientID: `1d164cd85549874d0e3a`,
		clientSecret: `527c3d223d1e6608953e835b547061037d140355`,
		repo: `HealerJean.github.io`,
		owner: 'HealerJean',
		admin: ['HealerJean'],
		id: 'AAAAAAAAAAAAAAAAAA',
    });
    gitalk.render('gitalk-container');
</script> 



<!-- Gitalk end -->



