---
title: 
date: 2025-01-01 00:00:00
tags: 
- 
category: 
- 
description: 
---

**前言**     

 Github：[https://github.com/HealerJean](https://github.com/HealerJean)         

 博客：[http://blog.healerjean.com](http://HealerJean.github.io)          





# 一、基础介绍

## 1、为什么选择`TiDB`

### **1）、架构与扩展性：从单机到分布式的突破**

#### **a、`MySQL` 的单机瓶颈与 `TiDB` 的分布式架构**

- `MySQL` 的局限性
  - 单机存储与计算能力有限，数据量超过 `TB` 级或并发量高时，性能会显著下降。
  - 分库分表需要手动设计与维护，增加开发复杂度（如跨库` Join`、事务一致性问题）。
- `TiDB` 的分布式优势
  - 采用 **`Shared-Nothing`  架构**，支持水平扩展（计算节点` TiDB Server`、存储节点 `TiKV`），理论上可扩展至 `PB` 级数据。
  - 自动数据分片（`Region `机制）与负载均衡，无需手动分库分表，开发成本低。
  - 在线事务处理( `TP` )底层存储采用行存, 在线分析( `AP` )底层存储采用列存, 相互隔离互不影响。业务可以像使用 `MySQL`一样简单的使用 `TiDB`, 无需分库分表的烦恼, 无需关心拆分键.

#### **b、弹性扩展能力**

- `MySQL`：扩展需通过主从复制、分库分表，但主从同步存在延迟，分表后难以应对复杂查询。
- `TiDB`：新增节点即可自动平衡数据与负载，支持在线扩缩容，对业务无中断。



### **2）性能：海量数据与高并发场景的优化**

#### **a、查询性能对比**

- 复杂查询与聚合场景
  - `MySQL`：单机查询时，亿级数据的 `JOIN`、分组统计（如 `COUNT`、`SUM`）易出现慢查询，需依赖索引优化或分库分表。
  - `TiDB`：利用分布式计算能力，将查询下推至多个 `TiKV` 节点并行处理，聚合性能提升 `10`-`100` 倍（例如：亿级数据分组统计可在秒级完成）。
- **示例**：对 `1` 亿条订单数据按日期 + 品类分组统计，`TiDB` 可通过分布式聚合快速返回结果，而 `MySQL` 可能因单机计算压力导致超时。

#### **b、写入与事务性能**

- `MySQL`：`InnoDB` 引擎在高并发写入时，受锁机制影响（如行锁、表锁），易出现性能瓶颈。
- `TiDB`：支持分布式事务（基于 `Percolator` 协议），通过多节点并行写入提升吞吐量，同时保证 `ACID` 一致性，适合交易类场景。



### **3）功能兼容性：`MySQL` 生态的无缝迁移**

#### **a、`SQL` 兼容性**

- `TiDB` 兼容 `MySQL 5.7` 协议与语法（如 `DDL`、`DML`、函数），应用程序无需大幅修改即可迁移，降低改造成本。
- 支持 `MySQL` 生态工具（如 `Navicat`、`MySQL` `Workbench`、`ORM` 框架），运维与开发习惯无缝衔接。

#### **b、生态集成与工具链**

- `MySQL` 生态成熟，但分布式扩展需额外组件（如 `MyCat`、`Sharding-JDBC`），稳定性与运维成本较高。
- `TiDB` 自带分布式集群管理工具（`TiUP`），支持自动化部署、监控与故障恢复，且兼容 `Prometheus`、`Grafana` 等监控体系。



### 4）**企业级需求：高可用、一致性与运维**

#### **a、高可用性架构**

- `MySQL`：主从复制架构下，主库故障需手动切换，存在数据丢失风险（异步复制）。
- `TiDB`：
  - 三副本保证数据强一致性，任意节点故障可自动选举新节点，服务不中断。
  - 支持多机房部署（如同城多活、异地灾备），满足金融级高可用需求。

#### **b、数据一致性与事务**

- `MySQL`：分布式事务需依赖 XA 协议，性能损耗大，实现复杂。
- `TiDB`：原生支持分布式强一致事务，适用于金融、电商等对事务要求高的场景（如订单支付、账户扣款）。

#### **c、运维与监控**

- `MySQL`：分布式场景下运维复杂度高（如分表管理、主从同步监控）。
- `TiDB`：自动化运维工具（`TiUP`）支持一键部署、扩容、缩容，内置监控面板（`TiDB` `Dashboard`）实时展示集群状态与性能指标。



### **5）与 `MySQL` 的成本对比**

- **硬件成本**：`TiDB` 需多节点部署，初期硬件投入高于单机 `MySQL`，但数据量超过 `TB` 级后，`MySQL` 分库分表的硬件与运维成本会反超 `TiDB`。
- **人力成本**：`TiDB` 减少分库分表开发与运维工作量，长期来看人力成本更低。

- **更少的资源占用：**相同数据量下 `TiDB` 存储的磁盘占用是 `MySQL` 的 `1/3` , 更加节省存储资源, 降本增效.



### 6）分库分表

#### a、**原生分布式能力**

- `MySQL`  分库分表：表结构设计时必须为每个表设计合适的分片键，并且在 `SQL` 查询时指定分片键，否则对集群的影响是灾难性的；不支持全局二级索引，必须加上分片键才能实现精确查询，牺牲了业务多维自定义查询能力，对业务有极高侵入性，对于开发有极大挑战，增加系统架构的复杂度。     
- `TiDB`：建表 + 使用（`SQL`）无需指定分片 / 区键，无业务侵入性；大幅提升开发效率；支持全局二级索引，提供多维自定义查询能力，与单机 `MySQL` 使用习惯一致。

#### **b、分布式事务**

- `MySQL` 分库分表：分库分表的分布式事务大部分基于弱一致性事务，需要外部组件来实现，或者需要业务来实现，对业务开发非常不友好。

- `TiDB`：`100%`  `ACID`（金融级保障），分布式事务性能良好，显著提升业务开发效率。

#### **c、在线水平横向扩展**

- `MySQL` 分库分表：分库分表架构在进行在线水平横向扩展时，非常麻烦，几乎所有的数据都要重新分布，对于开发和运维的压力巨大，业务连续性无保障。

- `TiDB`：存储计算分离架构，灵活性较高，存储和计算节点一键在线水平扩展，不影响业务，大幅缩短扩展实施周期。

- **在线 `DDL`**
  - `MySQL`  分库分表：`MySQL` 原生 `Online` `DDL` 能力较弱，原本在单表的 `DDL` 上对业务的影响较大（锁表），当一个表的分片数达到几十个时，`DDL` 操作对于运维压力巨大。

  - `TiDB`：原生 `Online DDL` 能力，在进行 `DDL` 操作时，完全不锁表，千亿级以上的表加减列秒级完成。

#### d、分析能力

- `MySQL` 分库分表：`MySQL` 原生能力比较差，使用分库分表架构后，对于复杂 `SQL` 的支持问题突出，需要引入外部分析引擎才能承载业务模块的基础分析需求。

- `TiDB`：基于 `MPP` 架构的列式存储引擎提供实时分析能力，能够在隔离性、实时性、一致性上得到保障，提供实时的分析。

#### e、高可用

- `MySQL` 分库分表：高可用切换需要外部组件或者人工进行干预，跨中心复制方案不完善。

- `TiDB`：内置 Raft 一致性协议，大多数副本存活的情况下可实现自恢复，支持同城多中心、双中心，两地三中心等金融级高可用方案。

#### f、服务支持

- `MySQL` 分库分表：中间件 + `MySQL` 方案缺乏原厂商业保障。

- `TiDB`：提供本地化的原厂商业服务支持。

#### g、建设成本

- `MySQL`分库分表：分库分表支持的业务负载单一，为支撑业务中台的业务需求，需要引入多个技术栈才能提供完整服务，建设成本高、周期长，开发运维复杂。

- `TiDB`：一套技术栈解决 `80%` 以上的业务需求，敏捷高效，建设与开发运维成本低。





# 二、架构



# 三、**选择 `TiDB` 的核心场景与时机**

## 1、基本对比

- **选 `TiDB`**：当业务需要**处理海量数据 + 复杂分析查询**，且对秒级响应可接受时，分布式计算优势显著。
- **选 `MySQL`**：当业务以**简单查询 + 低延迟交易**为主，数据量未触及单机瓶颈时，单机架构更高效。

| **场景维度**     | **`TiDB` 优势场景**                          | **`MySQL` 优势场景**                        |
| ---------------- | -------------------------------------------- | ------------------------------------------- |
| **数据规模**     | 亿级以上（分布式计算优势）                   | 千万级以下（单机性能足够）                  |
| **查询类型**     | 复杂聚合（`COUNT`/`SUM`/`JOIN`）、分析型查询 | 简单查询（单表主键查询）、`OLTP` 交易型查询 |
| **延迟敏感性**   | 秒级响应可接受（如报表统计）                 | 毫秒级响应（如支付扣款、秒杀下单）          |
| **计算资源需求** | 需要并行计算能力（多节点分摊压力）           | 单机计算资源足够（CPU / 内存利用率低）      |

**电商订单分析（复杂查询）**：`TiDB`：统计 “近 `30` 天各品类销售额  `TOP10`”，利用分布式聚合秒级返回结果；`MySQL` 单机可能因全表扫描超时，分库分表需跨库聚合，开发复杂且性能不稳定。   

**用户登录验证（简单查询）**：`MySQL`：查询用户信息（单表主键查询），响应时间 < `10ms`，满足实时性要求；`TiDB` 因跨节点调用延迟可能达 `20ms`，用户感知轻微卡顿。



## 2、推荐场景

### 1）应用场景总结

- **海量数据在线业务：**电商订单、金融交易记录（亿级数据），需实时查询与统计，`MySQL` 分库分表难以满足，`TiDB` 可直接应对。

- **高并发读写场景：**秒杀、社交平台 `Feed` 流，`TiDB` 的分布式架构可承载百万级 ` QPS`，避免 `MySQL` 单机瓶颈。

- **`OLTP` 与 `OLAP` 混合场景：**`TiDB` 支持 `HTAP`（混合事务与分析处理），无需数据同步至数仓，直接在生产库执行复杂分析查询。

- **业务快速扩展场景：**初创公司或业务增长迅速的企业，`TiDB` 的弹性扩展能力可避免频繁重构数据库架构。

### 2）适用条件

- 数据量预计快速增长至亿级以上，需避免分库分表。
- 要求高可用、强一致事务（如金融、支付）。
- 需同时支持  `OLTP`（交易）与 `OLAP`（分析）的混合场景。
- 希望降低分布式数据库开发与运维成本。



## 3、不推荐场景

- 数据量小（百万级以下）、业务逻辑简单，`MySQL` 单机即可满足。
- 对 `MySQL` 生态深度依赖（已使用分库分表、使用类似 `MyCat` 等中间件）
- 无扩展需求的传统业务。



# 四、查询的性能

## 1、**`TiDB` 在复杂查询 / 聚合场景的性能优势**

### **1）优势场景：亿级数据 + 复杂计算**

- **原理**：`TiDB` 通过分布式架构将查询拆分为多个子任务，在多个 `TiKV` 节点并行处理（如分组统计、多表 `JOIN`），利用多节点计算资源缩短整体耗时。
- **示例**：对 `1` 亿条订单数据按 “日期 + 品类” 分组统计总金额，`TiDB` 可将数据分片到 `10` 个节点并行计算，每个节点处理 `1000` 万条，最终合并结果，总耗时约 `1-2` 秒；而 `MySQL` 单机需扫描全量数据，耗时可能超过 `1` 分钟。
- **核心优势**：**并行计算能力解决了单机计算瓶颈**，尤其适合 `1OLAP` 分析型查询。



### **2）优势本质：分布式架构对 “计算密集型” 任务的优化**

- 复杂查询的瓶颈在于**计算能力**，`TiDB` 通过分布式扩展计算节点，将 ` O(n)` 的单机计算复杂度降为 `O(n/m)`（`m` 为节点数），性能随节点增加线性提升。

  

## **2、`TiDB` 在简单查询场景的性能劣势**

### **1）劣势场景：低延迟 + 简单查询**

- **原理**：`MySQL` 单节点查询直接访问本地磁盘，无需跨网络调用；而 `TiDB` 的简单查询（如单表主键查询）可能涉及跨节点路由（即使数据在同一节点，也需通过 `TiDB` `Server` 转发），引入额外网络延迟（约 5-10ms）。
- **示例**：查询单条用户数据（如 `SELECT * FROM user WHERE id=123`），`MySQL` 单节点响应时间约 `1-2ms`，`TiDB` 因跨组件调用（`TiDB Server`→`TiKV`→返回结果）可能耗时 `10-20ms`。
- **核心劣势**：**分布式架构的网络开销影响了简单查询的延迟**，尤其对毫秒级响应要求的场景（如高频交易）。



### **2）劣势本质：分布式架构对 “IO 密集型” 任务的额外开销**

- 简单查询的瓶颈在于**`IO` 延迟**，`TiDB` 的分布式链路（客户端→`TiDB Server`→`TiKV`→返回）比 `MySQL` 的单机链路（客户端→`MySQL Server`→本地磁盘→返回）多了网络交互步骤，放大了延迟。



# 五、**`TiDB` 数据倾斜问题解决方案**

## 1、**数据倾斜的本质与影响**

**本质**：数据在分布式集群中分布不均，导致部分 `TiKV` 节点负载过高（如存储量、读写请求数远超其他节点）。
**影响**：

- 热点节点成为性能瓶颈，导致整体集群吞吐量下降；
- `Raft` 日志同步延迟增加，影响事务一致性；
- 部分节点资源（`CPU`/`IO`/ 内存）利用率过高，可能触发服务降级。



## 2、数据倾斜的常见原因

`TiDB` 解决数据倾斜需从 **设计（表结构）、调度（`PD` 策略）、业务（流量打散）** 三层入手：

- **预防优先**：设计阶段避免自增主键、合理选择分片键；
- **动态调度**：通过 `PD` 自动迁移热点 `Region`，配合参数优化提升调度效率；
- **业务适配**：对高频操作添加缓存或分片查询，减轻集群压力。

通过上述方案，可有效将数据倾斜率（单节点负载 / 平均负载）控制在 1.5 倍以内，保障集群性能稳定性。

### 1）**表结构设计问题**

- 主键或分区键选择不当（如使用自增 `ID`、时间字段未分片），导致数据集中在少数 `Region`；
- 未创建合适的二级索引，查询或写入操作集中在特定数据范围。

### 2）集群调度策略不足

- `PD`（`Placement` `Driver`）调度参数未优化，`Region` 迁移不及时或策略不合理。

### 3）业务访问模式倾斜

- 高频操作（如更新、查询）集中在某段时间或某个值域（如订单表按日期查询近期数据）；
- 缓存失效或业务突发流量导致瞬时访问集中。



## 3、**`TiDB` 数据倾斜解决方案**

### **1） 表结构与索引优化**

#### **a、主键设计：避免自增 ID 与热点键**

- **反模式**：使用自增主键（如 `AUTO_INCREMENT`），导致新数据始终写入最后一个 Region。

- 优化方案

  - **哈希分片主键**：将主键与哈希值结合（如 `id + MOD(id, 100)`），打散数据分布；

  - **复合主键：**使用业务字段 + 随机数组合 

    ```sql
    CREATE TABLE t (
      user_id BIGINT,
      rand_num INT,
      -- 其他字段
      PRIMARY KEY (user_id, rand_num)
    );
    ```

  - **`UUID` 主键**：使用 `UUID()` 或 `UUID_TO_BIN()` 生成无序主键（需注意 `UUID` 索引查询效率）。

#### **b、分区表与分片规则**

- 范围分区（适用于时间序列数据）按时间字段分区（如年 / 月），并结合哈希分片避免单分区过热：

```sql
REATE TABLE t (
  id BIGINT,
  create_time DATETIME,
  -- 其他字段
  PRIMARY KEY (id, create_time)
) 
PARTITION BY RANGE COLUMNS(create_time) (
  PARTITION p2024 VALUES LESS THAN ('2025-01-01'),
  PARTITION p2025 VALUES LESS THAN ('2026-01-01'),
  -- 更多分区
);
```

- **哈希分区（适用于均匀分布场景）**：对高频访问字段添加哈希分片（如用户 ID）：

```sql
CREATE TABLE t (
  user_id BIGINT,
  -- 其他字段
  PRIMARY KEY (MOD(user_id, 100), user_id)
);
```

#### **c、二级索引优化**

- 对高频查询 / 更新字段创建索引时，避免索引键倾斜：
  - 反例：对 `status` 字段（值分布不均，如 `90%` 为 `active`）创建索引，导致查询集中在少数 `Region`；
  - 优化：添加随机后缀或组合字段（如 `(status, RAND())`），打散索引访问。



### 2）**集群调度与参数优化**

#### **a、`PD` 调度参数调整**

- 通过 `pd-ctl` 命令优化 `Region` 迁移策略

**开启负载均衡**：

```bash
pd-ctl config set enable-rebalance-hot-region true
pd-ctl config set hot-region-schedule-limit 4  # 热点 Region 调度并发数
```

**调整 `Region` 大小阈值**：

```bash
pd-ctl config set max-region-size 1048576  # 单位 KB，默认 96MB，可减小至 64MB 提升分片密度
pd-ctl config set max-region-keys 200000    # 单个 Region 最大键数，默认 20 万，可根据数据大小调整
```

**热点 `Region` 强制迁移**：

```bash
pd-ctl hot-region schedule table table_id  # 对指定表的热点 Region 触发调度
```

#### **b、`TiKV` 存储层优化**

- 调整 `TiKV` 存储参数，提升热点 `Region` 处理能力：

```toml
[rocksdb]
max-sub-compactions = 4  # 并发压缩任务数，提升写入性能
max-background-jobs = 20 # 后台任务数，默认 16，可增加至 20
```



### 3）**业务层面优化**

#### **a、读写请求打散**

- **查询场景**：对时间范围查询添加随机偏移（如查询近 `7` 天数据时，按用户 `ID` 分片查询）；
- **写入场景**：对高频写入表添加中间缓存（如 `Redis`），批量写入 `TiDB`（减少瞬时流量冲击）。



#### **b、冷热数据分离**

- 将高频访问的热数据与历史冷数据分表存储：
  - 热数据表：采用高效分片策略，部署在高性能节点；
  - 冷数据表：降低资源配置，或归档至 TiDB 归档集群（如使用 TiDB-Dump + 离线导入）。



### 5）主键、分区与二级索引

三者协同的本质是：**通过主键打散物理存储，分区管理逻辑分组，索引优化访问路径**，从 “存储分布→数据管理→查询效率” 形成闭环。

- **主键**：决定数据在分布式集群中的基础分布（全局打散）；
- **分区**：在主键分片基础上，进一步按业务维度（如时间、地域）分组，降低单分区数据规模；
- **二级索引**：优化查询路径，避免因索引键倾斜导致的访问集中。   



**问题1：`region` 和分区关系？**

可以把**分区表**看作 “文件夹”，**`Region` **看作 “文件夹里的文件块”：

1. 一个数据库表被分成多个 “文件夹”（分区 p2024、p2025），每个文件夹存放特定时间的数据。
2. 每个文件夹里的文件（数据）被进一步拆分成多个 “文件块”（`Region`），每个文件块分散存储在不同硬盘（TiKV 节点）上。
3. 主键打散相当于给每个文件块编号（MOD 运算），让文件块均匀分布在硬盘上，避免单个硬盘过载。
4. 二级索引相当于每个文件夹的 “目录”，查询时直接查对应文件夹的目录，快速定位文件块。



#### a、**主键**

> 主键：分布式存储的 “基石”—— 决定数据打散粒度

**案例：电商订单表**

- **反模式**：使用自增主键 `order_id`，新订单全部写入最后一个 `Region`，导致写入倾斜；

- 优化设计

  ```sql
  -- 主键 = 业务主键 + 哈希分片键
  PRIMARY KEY (order_id, MOD(order_id, 100))  
  ```

- 协同逻辑

  - 哈希分片键（`MOD(order_id, 100)`）将数据分散到 `100` 个逻辑桶中，每个桶对应独立 `Region`，避免写入集中；
  - 分区与索引基于此主键分布进一步优化（如按桶分区、对分片键创建索引）。



#### **b、分区**

> 分区：在主键分片上 “逻辑分组”—— 缩小倾斜影响范围

**场景：按时间维度分区**

- 表结构设计

  ```sql
  CREATE TABLE orders (
    order_id BIGINT,
    create_time DATETIME,
    -- 其他字段
    PRIMARY KEY (order_id, MOD(order_id, 100)),  -- 主键打散
    KEY idx_create_time (create_time)  -- 二级索引
  )
  PARTITION BY RANGE COLUMNS(create_time) (  -- 按时间分区
    PARTITION p2024 VALUES LESS THAN ('2025-01-01'),
    PARTITION p2025 VALUES LESS THAN ('2026-01-01')
  );
  ```

- 协同逻辑

  - **主键打散**：每个时间分区内的数据通过 `MOD(order_id, 100)` 分散到 `100` 个 `Region`，避免单分区内写入倾斜；
  - **分区隔离**：历史分区（如 p2024）与当前分区（p2025）物理隔离，热点仅影响当前分区，不波及全量数据；
  - **索引优化**：二级索引 `idx_create_time`  在每个分区内独立维护，查询时按分区过滤，减少跨分区扫描。



#### **c、二级索引**

> 二级索引：优化访问路径 —— 避免索引键导致的查询倾斜

场景：用户表按 `status` 字段高频查询

- **问题**：若 `status`  取值分布不均（如 `90%` 为 `active`），索引查询会集中访问少数 Region；

- 协同优化方案

  ```sql
  CREATE TABLE users (
    user_id BIGINT,
    status VARCHAR(20),
    rand_suffix INT,
    -- 主键与分区
    PRIMARY KEY (user_id, MOD(user_id, 50)),  -- 主键打散
    -- 带随机后缀的二级索引
    KEY idx_status (status, rand_suffix, user_id)  
  )
  PARTITION BY HASH(user_id) PARTITIONS 10;  -- 按用户 ID 哈希分区
  ```

- 协同逻辑

  - **主键与分区**：`user_id` 哈希分片确保用户数据均匀分布在 `50` 个 `Region` 和 `10` 个分区中；
  - **索引优化**：`status` 索引添加随机后缀 `rand_suffix`，将同类 `status` 的查询分散到不同 `Region`（如 `status='active'` 的查询会访问 `rand_suffix` 为` 0-99` 的多个 `Region`）；
  - **查询改写**：业务查询时添加随机后缀条件（如 `WHERE status='active' AND rand_suffix = FLOOR(RAND()*100)`），进一步打散流量。











![ContactAuthor](https://raw.githubusercontent.com/HealerJean/HealerJean.github.io/master/assets/img/artical_bottom.jpg)



<!-- Gitalk 评论 start  -->

<link rel="stylesheet" href="https://unpkg.com/gitalk/dist/gitalk.css">

<script src="https://unpkg.com/gitalk@latest/dist/gitalk.min.js"></script> 
<div id="gitalk-container"></div>    
 <script type="text/javascript">
    var gitalk = new Gitalk({
		clientID: `1d164cd85549874d0e3a`,
		clientSecret: `527c3d223d1e6608953e835b547061037d140355`,
		repo: `HealerJean.github.io`,
		owner: 'HealerJean',
		admin: ['HealerJean'],
		id: 'AAAAAAAAAAAAAAAAAA',
    });
    gitalk.render('gitalk-container');
</script> 



<!-- Gitalk end -->



