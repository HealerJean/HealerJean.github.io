---
title: AI之_2_大模型
date: 2025-11-14 00:00:00
tags: 
- AI
category: 
- AI
description: AI之_2_大模型
---

**前言**     

 Github：[https://github.com/HealerJean](https://github.com/HealerJean)         

 博客：[http://blog.healerjean.com](http://HealerJean.github.io)          

# 一、`Ollama`

> `Ollama` 是一个开源项目，旨在简化在本地设备（如笔记本电脑或个人服务器）上运行大型语言模型（LLMs）的过程。它最初由 Jeffrey Huang 和其他贡献者开发，目标是让开发者、研究人员和爱好者能够轻松地下载、运行和与各种开源大模型进行交互，而无需依赖云端服务。

## 1、主要特点：

- **本地运行**： `Ollama` 允许用户在自己的机器上直接运行 LLM，无需联网，保障数据隐私和安全性。
- **支持多种模型**： 支持包括 `Llama`、`Llama2`、`Llama3`、`Mistral`、`Gemma`、`Phi`、`Qwen` 等主流开源模型
- **一键部署**： 通过简单的命令（如 `ollama run llama3`），即可自动下载并运行指定模型，极大降低使用门槛。
- **跨平台支持**： 提供 `macOS`、Linux 和 Windows（通过 WSL 或原生支持）版本。
- **API 接口**： `Ollama` 提供 RESTful API，方便集成到其他应用中，例如 Web 应用、桌面工具或自动化脚本。
- **模型管理**： 用户可以轻松列出、拉取、删除和切换模型，例如：



## 2、模型管理

http://localhost:11434

```sh
ollama pull qwen:7b
ollama list
ollama rm llama2
```



## 3、`FAQ`

### 1）`Ollama` 的“强大管理能力”从何而来？

> `Ollama` 的“强大”主要体现在 **易用性、抽象封装和本地运行优化**，而不是依赖云端。具体来说：

- **模型已经“训练好了”**

  - 像 `qwen:7b` 这类模型，是在阿里云的超级计算机上用海量数据**提前训练好**的。

  - `Ollama` 只是把训练好的“成品大脑”（权重文件）下载到你电脑上。

  - 所以推理（即回答问题）时，**不需要再联网学习或查询**，直接用已有知识生成答案。

- **本地推理引擎高度优化**

  - `Ollama` 内部集成了高效的推理后端（如 llama.cpp），专门针对 CPU/GPU 做了优化。

  - 支持 Metal（`Mac`）、CUDA（NVIDIA）、AVX 等硬件加速，让大模型在笔记本上也能跑得动。

  - 所有计算都在你设备上完成 → **无需网络，保护隐私，响应快**。

- **“管理功能”是本地程序逻辑，不依赖云端**

  - `ollama list`、`ollama run`、`ollama rm` 这些命令只是在操作你本地的文件和进程。

  - 模型元数据、配置、缓存都存在你电脑上（如 `~/.ollama/` 目录）。

  - 就像 Docker 管理镜像一样：拉取一次，之后全本地操作。

- **API 也是本地服务**

  - 当你运行 `ollama serve`（默认自动启动），它会在 `http://localhost:11434` 启动一个本地 Web 服务。

  - 其他程序（如 Python 脚本、VS Code 插件）通过这个本地 API 与模型通信，**全程不经过互联网**。



### 2）为什么有些 `AI` 必须联网

> Ollama 的“强大”不是靠实时联网获取能力，而是：**把强大的 AI 模型“装进你的电脑”，让你拥有一个私有的、随时可用的 AI 助手。**

- **联网 `AI`**：像打电话问专家——每次都要拨号（联网），等对方回答。
- **`Ollama` 本地模型**：像请了一位专家住你家里——不用打电话，随时面对面交流。

| 类型                               | 是否需要联网   | 原因                                   |
| ---------------------------------- | -------------- | -------------------------------------- |
| **`Ollama`（本地 `LLM`）**         | ❌ 仅下载时需要 | 模型完整部署在本地                     |
| **`ChatGPT` / 文心一言 / 通义app** | ✅ 必须联网     | 模型在服务器上，你的输入要传到云端处理 |



### 3）**只有几个 GB 的文件，却能“记住”海量知识、回答各种问题**

> **几个 `GB` 不是“知识的总量”，而是“生成知识的能力”的压缩包**

| 原因             | 说明                                             |
| ---------------- | ------------------------------------------------ |
| **参数编码知识** | 知识以权重形式分布式存储在 70 亿参数中           |
| **高度压缩**     | **压缩冗余、提取共性**，用少量参数捕捉大量语义。 |
| **泛化能力**     | 学的是“规律”而非“原文”，能举一反三               |
| **语言冗余低**   | 模型只需捕捉核心语义，无需存储全部文本           |







# 二、









![ContactAuthor](https://raw.githubusercontent.com/HealerJean/HealerJean.github.io/master/assets/img/artical_bottom.jpg)



<!-- Gitalk 评论 start  -->

<link rel="stylesheet" href="https://unpkg.com/gitalk/dist/gitalk.css">

<script src="https://unpkg.com/gitalk@latest/dist/gitalk.min.js"></script> 
<div id="gitalk-container"></div>    
 <script type="text/javascript">
    var gitalk = new Gitalk({
		clientID: `1d164cd85549874d0e3a`,
		clientSecret: `527c3d223d1e6608953e835b547061037d140355`,
		repo: `HealerJean.github.io`,
		owner: 'HealerJean',
		admin: ['HealerJean'],
		id: 'AAAAAAAAAAAAAAAAAA',
    });
    gitalk.render('gitalk-container');
</script> 



<!-- Gitalk end -->



