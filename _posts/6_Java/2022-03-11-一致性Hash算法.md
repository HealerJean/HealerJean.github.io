---
title: 一致性Hash算法
date: 2022-03-11 00:00:00
tags: 
- Java
category: 
- Java
description: 一致性Hash算法
---

**前言**     

 Github：[https://github.com/HealerJean](https://github.com/HealerJean)         

 博客：[http://blog.healerjean.com](http://HealerJean.github.io)              

 来源：https://mp.weixin.qq.com/s/WTz1KA9kOGrqFVTtALJzjQ

# 1、一致性 Hash 算法背景

我们有三台缓存服务器编号`node0`、`node1`、`node2`，现在有 3000 万个`key`，希望可以将这些个 key 均匀的缓存到三台机器上，你会想到什么方案呢？          

我们可能首先想到的方案是：取模算法`hash（key）% N`，即：对 key 进行 hash 运算后取模，N 是机器的数量；                 

这样，对 `key` 进行 `hash` 后的结果对 `3 `取模，得到的结果一定是 `0`、`1` 或者 `2`，正好对应服务器`node0`、`node1`、`node2`，存取数据直接找对应的服务器即可，简单粗暴，完全可以解决上述的问题；



![image-20220311170900465](https://raw.githubusercontent.com/HealerJean/HealerJean.github.io/master/blogImages/image-20220311170900465.png)



## 1.1、缺点

取模算法虽然使用简单，但对机器数量取模，在集群扩容和收缩时却有一定的局限性：**因为在生产环境中根据业务量的大小，调整服务器数量是常有的事；**         

**而服务器数量 N 发生变化后`hash（key）% N`计算的结果也会随之变化！**

![image-20220311171001816](https://raw.githubusercontent.com/HealerJean/HealerJean.github.io/master/blogImages/image-20220311171001816.png)

**比如：一个服务器节点挂了，计算公式从`hash（key）% 3`变成了`hash（key）% 2`，结果会发生变化，此时想要访问一个 key，这个 key 的缓存位置大概率会发生改变，那么之前缓存 key 的数据也会失去作用与意义；**          

**大量缓存在同一时间失效，造成缓存的雪崩，进而导致整个缓存系统的不可用，这基本上是不能接受的；**        

为了解决优化上述情况，一致性 hash 算法应运而生~



# 2、一致性 Hash 算法详述

## 2.1、算法原理

> 一致性哈希算法在 1997 年由麻省理工学院提出，是一种特殊的哈希算法，在移除或者添加一个服务器时，能够尽可能小地改变已存在的服务请求与处理请求服务器之间的映射关系；       
>
> 解决：一致性哈希解决了简单哈希算法在分布式哈希表（Distributed Hash Table，DHT）中存在的动态伸缩等问题；       



一致性 hash 算法本质上也是一种取模算法； 不过，不同于上边按服务器数量取模，一致性 `hash` 是**对固定值 2^32 取模**；**IPv4 的地址是 4 组 8 位 2 进制数组成，所以用 2^32 可以保证每个 `IP` 地址会有唯一的映射；**



### 2.1.1、`hash`环

> 我们可以将这`2^32`个值抽象成一个圆环 ⭕️，圆环的正上方的点代表 0，顺时针排列，以此类推：1、2、3…直到 `2^32-1`，而这个由 2 的 32 次方个点组成的圆环统称为 `hash环`；

![image-20220311174712355](https://raw.githubusercontent.com/HealerJean/HealerJean.github.io/master/blogImages/image-20220311174712355.png)

### 2.1.2、服务器映射到 hash 环

> 在对服务器进行映射时，使用`hash（服务器ip）% 2^32`，即：**使用服务器 `IP` 地址进行 `hash` 计算，用哈希后的结果对`2^32`取模，结果一定是一个 0 到 `2^32-1` 之间的整数；**           
>
> **而这个整数映射在 hash 环上的位置代表了一个服务器，依次 将`node0`、`node1`、`node2`三个缓存服务器映射到 hash 环上；**



![image-20220311174826785](https://raw.githubusercontent.com/HealerJean/HealerJean.github.io/master/blogImages/image-20220311174826785.png)

### 2.1.3、对象 key 映射到服务器

> 在对对应的 Key 映射到具体的服务器时，需要首先计算 Key 的 Hash 值：`hash（key）% 2^32`；



**将 Key 映射至服务器遵循下面的逻辑：**      

**从缓存对象 key 的位置开始，沿顺时针方向遇到的第一个服务器，便是当前对象将要缓存到的服务器；**           

假设我们有 "semlinker"、"kakuqo"、"lolo"、"fer" 四个对象，分别简写为 o1、o2、o3 和 o4；        

首先，使用哈希函数计算这个对象的 hash 值，值的范围是 [0, 2^32-1]：

![image-20220311175617997](https://raw.githubusercontent.com/HealerJean/HealerJean.github.io/master/blogImages/image-20220311175617997.png)



```
hash(o1) = k1; 
hash(o2) = k2;
hash(o3) = k3; 
hash(o4) = k4;
```



同时 3 台缓存服务器，分别为 CS1、CS2 和 CS3：

![image-20220311175715072](https://raw.githubusercontent.com/HealerJean/HealerJean.github.io/master/blogImages/image-20220311175715072.png)'

```
K1 => CS1
K4 => CS3
K2 => CS2
K3 => CS1
```

![image-20220311175744699](https://raw.githubusercontent.com/HealerJean/HealerJean.github.io/master/blogImages/image-20220311175744699.png)



### 2.1.4、服务器缩容扩容

#### 2.1.4.1、缩容

> 假设 `CS3` 服务器出现故障导致服务下线，这时原本存储于 `CS3` 服务器的对象 `o4`，需要被重新分配至 `CS2` 服务器，其它对象仍存储在原有的机器上：**此时受影响的数据只有 CS2 和 CS3 服务器之间的部分数据！**



![image-20220311180248480](https://raw.githubusercontent.com/HealerJean/HealerJean.github.io/master/blogImages/image-20220311180248480.png)



#### 2.1.3.2、扩容

> 假如业务量激增，我们需要增加一台服务器 CS4，经过同样的 hash 运算，该服务器最终落于 t1 和 t2 服务器之间，具体如下图所示：     

![image-20220311180307924](https://raw.githubusercontent.com/HealerJean/HealerJean.github.io/master/blogImages/image-20220311180307924.png)

此时，只有 `t1` 和 `t2` 服务器之间的部分对象需要重新分配；         

在以上示例中只有 `o3` 对象需要重新分配，即它被重新到 `CS4` 服务器；         

在前面我们已经说过：如果使用简单的取模方法，当新添加服务器时可能会导致大部分缓存失效，而使用一致性哈希算法后，这种情况得到了较大的改善，因为只有少部分对象需要重新分配！        



## 2.2、数据偏斜&服务器性能平衡问题

> 在上面给出的例子中，各个服务器几乎是平均被均摊到 Hash 环上；但是在实际场景中很难选取到一个 Hash 函数这么完美的将各个服务器散列到 Hash 环上；此时，在服务器节点数量太少的情况下，很容易因为**节点分布不均匀而造成数据倾斜问题；**       
>
> 
>
> 问题1：如下图被缓存的对象大部分缓存在`node-4`服务器上，导致其他节点资源浪费，系统压力大部分集中在`node-4`节点上，这样的集群是非常不健康的：      
>
> 问题2：在上面新增服务器 CS4 时，CS4 只分担了 CS1 服务器的负载，服务器 CS2 和 CS3 并没有因为 CS4 服务器的加入而减少负载压力；如果 CS4 服务器的性能与原有服务器的性能一致甚至可能更高，那么这种结果并不是我们所期望的；

![image-20220311182124167](https://raw.githubusercontent.com/HealerJean/HealerJean.github.io/master/blogImages/image-20220311182124167.png)

### 2.2.1、虚拟节点

> **针对上面的问题，我们可以通过：引入虚拟节点来解决负载不均衡的问题：**
>
> **即将每台物理服务器虚拟为一组虚拟服务器，将虚拟服务器放置到哈希环上，如果要确定对象的服务器，需先确定对象的虚拟服务器，再由虚拟服务器确定物理服务器；**

![image-20220311182537191](https://raw.githubusercontent.com/HealerJean/HealerJean.github.io/master/blogImages/image-20220311182537191.png)

**在图中：o1 和 o2 表示对象，v1 ~ v6 表示虚拟服务器，s1 ~ s3 表示实际的物理服务器；**        



### 2.2.2、虚拟节点的计算

虚拟节点的 `hash` 计算通常可以采用：**对应节点的 `IP` 地址加数字编号后缀 `hash`（10.24.23.227#1) 的方式；**          

举个例子，node-1 节点  IP 为 10.24.23.227，正常计算`node-1`的 hash 值：     

⬤ `hash（10.24.23.227#1）% 2^32`

假设我们给 node-1 设置三个虚拟节点，`node-1#1`、`node-1#2`、`node-1#3`，对它们进行 hash 后取模：         

⬤ `hash（10.24.23.227#1）% 2^32`      

⬤ `hash（10.24.23.227#2）% 2^32`      

⬤ `hash（10.24.23.227#3）% 2^32`

> ⬤ **分配的虚拟节点个数越多，映射在 `hash` 环上才会越趋于均匀，节点太少的话很难看出效果；**     
>
> ⬤ **引入虚拟节点的同时也增加了新的问题，要做虚拟节点和真实节点间的映射，`对象key->虚拟节点->实际节点`之间的转换；**



### 2.2.3、使用场景

> 一致性 `hash` 在分布式系统中应该是实现负载均衡的首选算法，它的实现比较灵活，既可以在客户端实现，也可以在中间件上实现，比如日常使用较多的缓存中间件`memcached`和`redis`集群都有用到它；

其它的应用场景还有很多：     

方法   `RPC`框架`Dubbo`用来选择服务提供者

- 分布式关系数据库分库分表：数据与节点的映射关系
- `LVS`负载均衡调度器
- ……













![ContactAuthor](https://raw.githubusercontent.com/HealerJean/HealerJean.github.io/master/assets/img/artical_bottom.jpg)



<!-- Gitalk 评论 start  -->

<link rel="stylesheet" href="https://unpkg.com/gitalk/dist/gitalk.css">

<script src="https://unpkg.com/gitalk@latest/dist/gitalk.min.js"></script> 
<div id="gitalk-container"></div>    
 <script type="text/javascript">
    var gitalk = new Gitalk({
		clientID: `1d164cd85549874d0e3a`,
		clientSecret: `527c3d223d1e6608953e835b547061037d140355`,
		repo: `HealerJean.github.io`,
		owner: 'HealerJean',
		admin: ['HealerJean'],
		id: 'ptFyOl1CuaZ4UfLW',
    });
    gitalk.render('gitalk-container');
</script> 




<!-- Gitalk end -->



